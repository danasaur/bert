{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transfer learning (<-Part 1):\n",
    "- What is fine-tuning? \n",
    "- Why fine-tune vs starting from scratch? \n",
    "- How to choose a base model (high level - Why bert?)\n",
    "\n",
    "#### Why Bert? (High Level)\n",
    "\n",
    "#### DistilBert - Why are we fine-tuning this model instead?\n",
    "\n",
    "DistilBERT is a small, fast, cheap and light Transformer model trained by distilling BERT base. It has 40% less parameters than bert-base-uncased, runs 60% faster while preserving over 95% of BERT's performances as measured on the GLUE language understanding benchmark.\n",
    "\n",
    "It will be faster for us to train our model on DistilBERT. Best practice to create a model quickly to establish a baseline performance and iteratively add complexity if needed.  \n",
    "\n",
    "#### Intro to Hugging Face\n",
    "\n",
    "#### Fine tuning steps\n",
    "- create a dataset (convert from pandas to a hugging face dataset)\n",
    "- tokenize your training data with the same tokenizer used by the base model you are fine-tuning\n",
    "- \n",
    "\n",
    "#### Alternative fine-tuning methods (high level and resources for further learning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transfer learning High Level (ppt)\n",
    "\n",
    "# History of Transfer Learning (ppt)\n",
    "\n",
    "# Fine Tuning Approach High Level (ppt)\n",
    "\n",
    "# Models for NLP Fine Tuning (ppt)  \n",
    "- show timeline, pros and cons and use cases for each model\n",
    "\n",
    "# Intro to Hugging Face (ppt)\n",
    "\n",
    "# Show models on the hugging face hub (browser)  \n",
    "\n",
    "# Show the datasets on hugging face hub (browser)\n",
    "\n",
    "# Intro our use-case dataset and ask the students to load the dataset (colab)\n",
    "\n",
    "# Exercise: Load the dataset (colab) and evaluate the dataset \n",
    "# - show the first 5 rows, the number of rows, the number of columns, the column names, the column types, the number of unique values in each column, the number of missing values in each column\n",
    "- ask the students to find the number of unique values in the 'label' column and paste it into the colab notebook\n",
    "- ask the students to find the number of missing values in the 'label' column and paste it into the colab notebook\n",
    "- check the distribution of the labels\n",
    "- check the distribution of the text lengths\n",
    "- check the distribution of the number of words in the text\n",
    "- check the vocabulary size and the most common words and the least common words\n",
    "\n",
    "# Discuss which model to use as a base model (ppt)\n",
    "\n",
    "# ask the students to find the DistilBERT model name and tokenizer name (distilbert-base-uncased) and paste it into the colab notebook\n",
    "\n",
    "# Exercise: evaluate the model's vocabulary with respect to our dataset (colab)\n",
    "- show intersection, difference and symmetric difference between the model's vocabulary and our dataset's vocabulary\n",
    "- Is the model's vocabulary sufficient for our dataset?\n",
    "- Is the model's vocabulary too large for our dataset?\n",
    "- Ideally, we want the model's vocabulary to be a subset of our dataset's vocabulary (intersection) and the model's vocabulary to be as small as possible (difference) because the model's vocabulary is the number of parameters that need to be trained and the smaller the vocabulary, the faster the training and the smaller the model size.\n",
    "\n",
    "# Exercise: Load the model and tokenizer (colab)\n",
    "\n",
    "# Exercise: Tokenize the dataset (colab)\n",
    "\n",
    "# Exercise: Evaluate the tokenized dataset (colab) \n",
    "# - use the same code in previous notebook to check the distribution of the tokenized text lengths \n",
    "- Ideally, the embedding size of the model should be large enough to capture the information in the tokenized text but not too large because the model will waste time and resources to train the padding tokens. \n",
    "\n",
    "If the limit of the embedding size is 512, then the tokenized text should be no longer than 512 tokens. If the tokenized text is longer than 512 tokens, then the model will truncate the text and the model will not be able to capture the information in the truncated text. If the tokenized text is shorter than 512 tokens, then the model will pad the text with special tokens (e.g. [PAD]).\n",
    "\n",
    "# Take home Suggested Exercise: Compare the models' vocabularies (colab)\n",
    "# ask students to pick another model and find the model name and tokenizer name (e.g. bert-base-uncased, bert-base-uncased) and paste into the colab\n",
    "\n",
    "# Fine Tuning Steps (ppt outline the steps)\n",
    "\n",
    "# Exercise: Fine Tune DistilBERT (colab)\n",
    "\n",
    "\n",
    "\n",
    "#### Alternative fine-tuning methods (high level and resources for further learning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Models for NLP Fine Tuning (ppt)  \n",
    "- BERT, DistilBERT, RoBERTa, XLNet, XLM, CTRL, and T5 and their differences\n",
    "- BERT is the most popular and widely used model for NLP tasks\n",
    "- DistilBERT is a smaller version of BERT that is faster and uses less memory\n",
    "- RoBERTa is a variant of BERT that is trained with a different objective function\n",
    "- XLNet is a transformer model that uses a permutation language modeling objective\n",
    "- XLM is a variant of BERT that is multilingual\n",
    "- CTRL is a transformer model that uses a causal language modeling objective\n",
    "- T5 is a transformer model that uses a text-to-text objective\n",
    "\n",
    "They all have different tradeoffs and are useful for different tasks.\n",
    "    - BERT pros: Most powerful, most popular, most used, most research (most research means more papers, more papers means more people using it)\n",
    "    - BERT cons: Slow, large, memory intensive\n",
    "    - DistilBERT pros: Fast, small, memory efficient\n",
    "    - DistilBERT cons: Less powerful, less research\n",
    "    - RoBERTa pros: Fast, small, memory efficient, more research\n",
    "    - RoBERTa cons: Less powerful, less popular, less used\n",
    "    - XLNet pros: Fast, small, memory efficient, more research\n",
    "    - XLNet cons: Less powerful, less popular, less used\n",
    "    - XLM pros: Fast, small, memory efficient, more research, multilingual\n",
    "    - XLM cons: Less powerful, less popular, less used\n",
    "    - CTRL pros: Fast, small, memory efficient, more research\n",
    "    - CTRL cons: Less powerful, less popular, less used\n",
    "    - T5 pros: Fast, small, memory efficient, more research\n",
    "    - T5 cons: Less powerful, less popular, less used\n",
    "\n",
    "\n",
    "# How to decide which base model to use for fine-tuning?\n",
    "- If you have a lot of data, use BERT\n",
    "- If you have a little data, use DistilBERT\n",
    "- If you have a lot of data and want to use a multilingual model, use XLM\n",
    "- If you have a lot of data and want to use a model that is trained on a causal language modeling objective, use CTRL (this is a very specific use case) You may want to train on a causal language modeling objective if you are trying to predict the next word in a sentence or the next sentence in a paragraph or the next paragraph in a document or the next document in a corpus\n",
    "- Use T5 if you are trying to do a text-to-text task like summarization or translation\n",
    "- Use RoBERTa if you want to use a model that is trained with a different objective function. ??\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 2 motivation: we want to ask for reviews, but only from people who we predict will give us 5 stars... So if we can first predict the sentiment of a review to our internal private system, then we can only ask for reviews from people who we predict will give us 5 stars publicly.\n",
    "\n",
    "\n",
    "\n",
    "# Part 3: Can we express ourselves differently and maintain the same sentiment?\n",
    "# Create new datasets with chatgpt (colab)\n",
    "- create yelp reviews that maintain the same sentiment as the original yelp review but change the text to be more polite, succinct, sarcastic, compelling, persuasive, emotional, objective, professional\n",
    "- create yelp reviews that maintain the same sentiment as the original yelp review but change the text to be a limerick, haiku or sonnet\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
