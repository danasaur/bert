{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C0fype_qEZUf"
      },
      "source": [
        "# bert-based-uncased\n",
        "\n",
        "[See the model card for more information on the huggingface hub](https://huggingface.co/bert-base-uncased)\n",
        "\n",
        "[Google's release on github and associated bert models](https://github.com/google-research/bert/blob/master/README.md) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vJtpwxYREZUg"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "grKokc21EZUh",
        "outputId": "c54a0654-a3ab-46d0-ace3-c6004c006e0b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
            "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForMaskedLM, pipeline\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "model = AutoModelForMaskedLM.from_pretrained(\"bert-base-uncased\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VulLk-xtEZUi"
      },
      "source": [
        "## Optional Exercise: Play with the tokenizer!\n",
        "\n",
        "See how different words get tokenized  \n",
        "\n",
        "Suggestions to try:\n",
        "- your name\n",
        "- compound words\n",
        "- obscure words you think it might not know\n",
        "- your favorite something, food, movie, childhood toy\n",
        "- something from your culture"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OkIGsxo0EZUi",
        "outputId": "7451e5d3-fea0-4e9f-8830-d185348b5bab"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['dana', 'eng', '##eb', '##ret', '##son']"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenizer.tokenize(\"Dana Engebretson\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OAITkYr7EZUj"
      },
      "source": [
        "# Masked Language Modeling\n",
        "\n",
        "Here you can play with one of the \"dummy tasks\" BERT learned from. Let's pretend to be BERT learning a language and try to guess the masked word in the following sentence."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qKSLT4FJEZUj",
        "outputId": "d5a457ae-d0af-4f32-d8cc-6d03a43bb4a0"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
            "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ],
      "source": [
        "unmasker = pipeline('fill-mask', model='bert-base-uncased')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cyPt1IjnEZUj"
      },
      "outputs": [],
      "source": [
        "# help(pipeline)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kJ04mW5fEZUj",
        "outputId": "aaf22888-f903-4f22-8329-b5b42aa80717"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "UsageError: unrecognized arguments: comment out this line to see the results of the model\n"
          ]
        }
      ],
      "source": [
        "%%capture  # comment out this line to see the results of the model\n",
        "text = \"She craved [MASK] to quench her thirst.\"\n",
        "unmasker(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uo-bCtEYEZUj",
        "outputId": "b363ed07-62e2-491d-8e94-d50d39e2d27e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "UsageError: unrecognized arguments: comment out this line to see the results of the model\n"
          ]
        }
      ],
      "source": [
        "%%capture # comment out this line to see the results of the model\n",
        "text = \"The best thing about [MASK] is they mostly clean up after themselves!\"\n",
        "unmasker(text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eGMXHw-EEZUj"
      },
      "source": [
        "Ok, those are very interesting guesses..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wVzG2G05EZUk"
      },
      "source": [
        "# What data was this model trained on? "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UzTtkzEvEZUk"
      },
      "source": [
        "After some meandering on the hugging face hub, we discovered this paper, which does a deep dive investigation into one of the data sources that BERT was trained on. \n",
        "\n",
        "[Addressing Documentation Debt\" in Machine Learning Research: A Retrospective Datasheet for BookCorpus](https://arxiv.org/pdf/2105.05241.pdf)  \n",
        "\n",
        "And turns out, Vampire books account for 5.4% of the BookCorpus dataset üßõüèª‚Äç‚ôÄÔ∏è\n",
        "\n",
        "![image.png](attachment:image.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uklZF4H-EZUk"
      },
      "source": [
        "## Optional Exercise:\n",
        "\n",
        "Create new masked sentences and see if you can derive any other interesting insights into how this model \"models\" language. Let us know if you discover any surprising results!\n",
        "\n",
        "Suggestions:\n",
        "- Try some sentences with references to your culture, holidays, food, etc.\n",
        "- Try some sentences about historical events or people\n",
        "- Try some sentences about scientific discoveries or any field of interest to you\n",
        "- Try making up a joke, a song lyric or a short poem"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eyCJU7FzEZUk",
        "outputId": "794d55ae-ec82-4054-c7e9-a14bbb72dfce"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[{'score': 0.19289492070674896,\n",
              "  'token': 2126,\n",
              "  'token_str': 'way',\n",
              "  'sequence': 'give the model a way to predict.'},\n",
              " {'score': 0.051731377840042114,\n",
              "  'token': 2051,\n",
              "  'token_str': 'time',\n",
              "  'sequence': 'give the model a time to predict.'},\n",
              " {'score': 0.051390860229730606,\n",
              "  'token': 2193,\n",
              "  'token_str': 'number',\n",
              "  'sequence': 'give the model a number to predict.'},\n",
              " {'score': 0.03003263846039772,\n",
              "  'token': 3382,\n",
              "  'token_str': 'chance',\n",
              "  'sequence': 'give the model a chance to predict.'},\n",
              " {'score': 0.02129087597131729,\n",
              "  'token': 7774,\n",
              "  'token_str': 'curve',\n",
              "  'sequence': 'give the model a curve to predict.'}]"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# YOUR CODE HERE\n",
        "# Create a prompt text with some words masked out, see what the model predicts\n",
        "text = \"Give the model a [MASK] to predict.\"\n",
        "unmasker(text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sTwr4CBbEZUl"
      },
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "bert",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    },
    "orig_nbformat": 4,
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}