{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Digging into the data Bert was trained on"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From [the hugging face model card for bert-base-uncased](https://huggingface.co/bert-base-uncased) we can see info about the datasets BERT was trained on and after reading the paper [Addressing Documentation Debt\" in Machine Learning Research: A Retrospective Datasheet for BookCorpus](https://arxiv.org/pdf/2105.05241.pdf) it seems like a worthy cause to contribute to further analysis, for this model and others. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BookCorpus and BookCorpusOpen"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hugging face links to the [homepage for the original authors of BERT](https://yknzhu.wixsite.com/mbweb), But they aren't hosting the original dataset anymore.\n",
    "\n",
    "[The dataset card for BookCorpus](https://huggingface.co/datasets/bookcorpus) is a reproduction but it doesn't include meta data regarding the books the content comes from."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional Exercise: Extending the Analysis via the BookCorpusOpen Dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[The dataset card for BookCorpusOpen](https://huggingface.co/datasets/bookcorpusopen)\n",
    "\n",
    "This is also an imperfect reproduction, though it has meta data so there is opportunity to extend the paper [Addressing Documentation Debt\" in Machine Learning Research: A Retrospective Datasheet for BookCorpus](https://arxiv.org/pdf/2105.05241.pdf) with further analysis. \n",
    "\n",
    "Perhaps we could email the authors with copyrighted materials, try to crowdfund some money for them since many companies are making a lot of money from bert-based models that were trained on their hard work without their permission? \n",
    "\n",
    "Or explore the dataset to see if you can find any other insights. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset bookcorpusopen (/Users/d/.cache/huggingface/datasets/bookcorpusopen/plain_text/1.0.0/98559c92eb612e150a676c5b5131f9f8f07d4cab88e7f3761fda266ad22ff2a7)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a4e1487b5374a77886d815c653b265a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "\n",
    "dataset = load_dataset(\"bookcorpusopen\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(dataset[\"train\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def get_email_addresses(text):\n",
    "    return re.findall('[0-9a-zA-z]+@[0-9a-zA-z]+\\.[0-9a-zA-z]+', text)\n",
    "\n",
    "def get_copyright(text):\n",
    "    return re.findall('Copyright\\s*\\d*', text)\n",
    "\n",
    "def get_license_notes(text):\n",
    "    return re.findall('License Notes\\n\\n(.*?)(?:\\n)', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"email_addresses\"] = df[\"text\"].apply(get_email_addresses)\n",
    "df[\"copyright\"] = df[\"text\"].apply(get_copyright)\n",
    "df[\"license_notes\"] = df[\"text\"].apply(get_license_notes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df filtered where email_addresses is not empty, license_notes is not empty, and copy right is not empty\n",
    "df_notgood = df[(df[\"email_addresses\"].str.len() > 0) & (df[\"license_notes\"].str.len() > 0) & (df[\"copyright\"].str.len() > 0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>email_addresses</th>\n",
       "      <th>copyright</th>\n",
       "      <th>license_notes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>satan-the-sworn-enemy-of-mankind.epub.txt</td>\n",
       "      <td>\\n##  SATAN:\\n\\n## The Sworn Enemy of Mankind...</td>\n",
       "      <td>[info@harunyahya.com]</td>\n",
       "      <td>[Copyright ]</td>\n",
       "      <td>[## All rights reserved. No part of this publi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>crown-of-thorns-the-race-to-clone-jesus-christ...</td>\n",
       "      <td>\\n### Crown of Thorns -\\n\\n### The Race To Cl...</td>\n",
       "      <td>[iancpirvine@hotmail.co, iancpirvine@hotmail.co]</td>\n",
       "      <td>[Copyright 2001]</td>\n",
       "      <td>[This book is licensed for your personal enjoy...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>shattered.epub.txt</td>\n",
       "      <td>\\n\\nSHATTERED\\n\\nby\\n\\nSandra Madera\\n\\nEdited...</td>\n",
       "      <td>[smadera23@yahoo.com]</td>\n",
       "      <td>[Copyright ]</td>\n",
       "      <td>[This ebook is licensed for your personal enjo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>a-course-in-miracles-wth-comments-by-ron-rasmu...</td>\n",
       "      <td>\\n\\n### A Course in Miracles with Comments\\n\\n...</td>\n",
       "      <td>[ARequiredCourse@yahoogroups.com, Rasmussen@ao...</td>\n",
       "      <td>[Copyright 2015]</td>\n",
       "      <td>[This ebook is licensed for your personal enjo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>red-warp.epub.txt</td>\n",
       "      <td>\\nRed Warp\\n\\nDon DeBon\\n\\nStandard Edition\\n\\...</td>\n",
       "      <td>[debon@gmail.com]</td>\n",
       "      <td>[Copyright , Copyright ]</td>\n",
       "      <td>[This e-book is licensed for your personal enj...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                title   \n",
       "5           satan-the-sworn-enemy-of-mankind.epub.txt  \\\n",
       "12  crown-of-thorns-the-race-to-clone-jesus-christ...   \n",
       "27                                 shattered.epub.txt   \n",
       "33  a-course-in-miracles-wth-comments-by-ron-rasmu...   \n",
       "60                                  red-warp.epub.txt   \n",
       "\n",
       "                                                 text   \n",
       "5    \\n##  SATAN:\\n\\n## The Sworn Enemy of Mankind...  \\\n",
       "12   \\n### Crown of Thorns -\\n\\n### The Race To Cl...   \n",
       "27  \\n\\nSHATTERED\\n\\nby\\n\\nSandra Madera\\n\\nEdited...   \n",
       "33  \\n\\n### A Course in Miracles with Comments\\n\\n...   \n",
       "60  \\nRed Warp\\n\\nDon DeBon\\n\\nStandard Edition\\n\\...   \n",
       "\n",
       "                                      email_addresses   \n",
       "5                               [info@harunyahya.com]  \\\n",
       "12   [iancpirvine@hotmail.co, iancpirvine@hotmail.co]   \n",
       "27                              [smadera23@yahoo.com]   \n",
       "33  [ARequiredCourse@yahoogroups.com, Rasmussen@ao...   \n",
       "60                                  [debon@gmail.com]   \n",
       "\n",
       "                   copyright   \n",
       "5               [Copyright ]  \\\n",
       "12          [Copyright 2001]   \n",
       "27              [Copyright ]   \n",
       "33          [Copyright 2015]   \n",
       "60  [Copyright , Copyright ]   \n",
       "\n",
       "                                        license_notes  \n",
       "5   [## All rights reserved. No part of this publi...  \n",
       "12  [This book is licensed for your personal enjoy...  \n",
       "27  [This ebook is licensed for your personal enjo...  \n",
       "33  [This ebook is licensed for your personal enjo...  \n",
       "60  [This e-book is licensed for your personal enj...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_notgood.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wikipedia "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional Exercise: \n",
    "\n",
    "I'm not sure if this is the original wikipedia dataset that BERT was trained on, but perhaps it'd be informative to extend the paper [Addressing Documentation Debt\" in Machine Learning Research: A Retrospective Datasheet for BookCorpus](https://arxiv.org/pdf/2105.05241.pdf) with further analysis on this wikipedia dataset? \n",
    "\n",
    "[The hugging face dataset card for wikipedia](https://huggingface.co/datasets/wikipedia)\n",
    "\n",
    "Can you aggregate the data and create a data card that reports the percentage of the data that fits under various categories between the BookCorpus and Wikipedia datasets?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bert",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
